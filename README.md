# Houses Rental Scraping ETL Pipeline

A well-architected ETL (Extract, Transform, Load) pipeline for scraping rental property data from Espacio Urbano website.

## ğŸ—ï¸ Architecture

The project follows a clean ETL architecture with three distinct layers:

### ğŸ“¥ Extract Layer (`etl/extract/`)
- **`BaseExtractor`**: Abstract base class for all extractors
- **`CitiesExtractor`**: Extracts city information from the main listings page
- **`AnnouncementsExtractor`**: Extracts rental property announcements from city pages

### ğŸ”„ Transform Layer (`etl/transform/`)
- **`BaseTransformer`**: Abstract base class for all transformers
- **`CitiesTransformer`**: Cleans and standardizes city data
- **`AnnouncementsTransformer`**: Cleans and standardizes property announcement data

### ğŸ’¾ Load Layer (`etl/load/`)
- **`BaseLoader`**: Abstract base class for all loaders
- **`CSVLoader`**: Saves data to CSV files with various options

### ğŸ¯ Orchestration (`etl/etl_orchestrator.py`)
- **`ETLOrchestrator`**: Coordinates the entire pipeline execution

## ğŸš€ Quick Start

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd houses-rental-scraping
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Running the Pipeline

#### Option 1: Run the complete pipeline
```bash
python main.py
```

#### Option 2: Use the CLI interface
```bash
# Run complete pipeline
python cli.py full

# Run only cities extraction
python cli.py cities

# Show error summary from last run
python cli.py errors
```

## ğŸ“ Project Structure

```
houses-rental-scraping/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py          # Configuration settings with hardcoded URLs
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ etl_orchestrator.py  # Main ETL orchestrator
â”‚   â”œâ”€â”€ extract/             # Extraction layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_extractor.py
â”‚   â”‚   â”œâ”€â”€ cities_extractor.py
â”‚   â”‚   â””â”€â”€ announcements_extractor.py
â”‚   â”œâ”€â”€ transform/           # Transformation layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_transformer.py
â”‚   â”‚   â”œâ”€â”€ cities_transformer.py
â”‚   â”‚   â””â”€â”€ announcements_transformer.py
â”‚   â””â”€â”€ load/               # Loading layer
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base_loader.py
â”‚       â””â”€â”€ csv_loader.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py           # Logging configuration
â”œâ”€â”€ main.py                 # Main entry point
â”œâ”€â”€ cli.py                  # Command-line interface
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md              # This file
```

## âš™ï¸ Configuration

All configuration is centralized in `config/settings.py`:

- **URLs**: Base URLs for scraping (hardcoded for reliability)
- **Scraping Parameters**: Page limits, sleep times, timeouts
- **Data Storage**: Output directories and filenames
- **Property Types**: Mapping of property type codes

## ğŸ“Š Output Data

The pipeline generates the following CSV files:

### `data/cities.csv`
Contains city information:
- `id`: Unique city identifier
- `city_name`: Cleaned city name
- `url`: Source URL
- `is_active`: Whether the city is active
- `created_at`: Timestamp

### `data/announcements.csv`
Contains rental property announcements:
- `announcement_id`: Unique announcement identifier
- `url`: Link to full announcement
- `city_name`: Associated city
- `neighborhood`: Property neighborhood
- `price`: Monthly rent price
- `rooms`: Number of rooms
- `bathrooms`: Number of bathrooms
- `parkings`: Number of parking spaces
- `area`: Property area in square meters
- `description`: Property description
- `property_type`: Inferred property type
- `created_at`: Timestamp

### `data/announcements_{city_name}.csv`
Individual files for each city's announcements.

## ğŸ” Data Sources

The pipeline scrapes data from:
1. **Cities List**: `https://www.espaciourbano.com/listado_arriendos.asp`
2. **City Announcements**: `https://www.espaciourbano.com/Resumen_Ciudad_arriendos.asp`

## ğŸ“ Logging

Logs are automatically saved to `data/logs/` with timestamps. The logging system provides:
- File and console output
- Detailed error tracking
- Performance monitoring
- URL failure tracking

## ğŸ› ï¸ Development

### Adding New Extractors
1. Inherit from `BaseExtractor`
2. Implement the `extract()` method
3. Add to the orchestrator

### Adding New Transformers
1. Inherit from `BaseTransformer`
2. Implement the `transform()` method
3. Add data cleaning logic

### Adding New Loaders
1. Inherit from `BaseLoader`
2. Implement the `load()` method
3. Add to the orchestrator

## ğŸš¨ Error Handling

The pipeline includes comprehensive error handling:
- Retry mechanisms for failed requests
- Graceful handling of parsing errors
- Detailed error logging and reporting
- Non-blocking failures (continues processing other data)

## ğŸ“ˆ Performance

- **Concurrent Processing**: Uses ThreadPoolExecutor for parallel city processing
- **Rate Limiting**: Configurable sleep times between requests
- **Memory Efficient**: Processes data in batches
- **Error Recovery**: Continues processing even if some URLs fail

## ğŸ”’ Legal Considerations

This tool is designed for private use and research purposes. Users are responsible for:
- Complying with the website's terms of service
- Respecting robots.txt guidelines
- Implementing appropriate rate limiting
- Using the data responsibly

## ğŸ“ Support

For issues or questions:
1. Check the logs in `data/logs/`
2. Review the error summary using `python cli.py errors`
3. Verify configuration in `config/settings.py`